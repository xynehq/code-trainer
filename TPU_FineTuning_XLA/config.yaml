# CPT Configuration for Hyperswitch Rust Repository
# =================================================

# Repository Configuration
repository:
  name: "hyperswitch"
  path: "/home/aditya.narayan/TPU_FineTuning/hyperswitch"
  language: "rust"

# Model Configuration
model:
  name: "Qwen/Qwen2.5-Coder-7B-Instruct"
  size: "7B"
  type: "instruct"  # chat/instruct model with conversational capabilities

# TPU Configuration (8 x v6e TPUs)
tpu:
  type: "v6e"
  topology: "2x2x1"              # 8 chips = 2x2x1 topology
  num_devices: 8
  
  # TPU runtime settings
  runtime:
    pjrt: true                   # Use PJRT runtime
    precompile: true             # Precompile operations
    
  # Memory optimization for TPUs
  memory:
    host_to_device_transfer_threads: 4
    
# Weights & Biases Configuration
wandb:
  enabled: true
  api_key: "WANDB-API-KEY"
  project: "hyperswitch-cpt"
  entity: null                   # Set to your wandb username/team if needed
  run_name: "qwen2.5-coder-7b-hyperswitch"
  log_model: true                # Log model checkpoints to wandb
  log_interval: 10               # Log every N steps
  watch_model: false             # Watch gradients (can be expensive on TPU)

# Training Configuration
training:
  # LoRA settings
  lora:
    r: 16                    # Reduced to 16 for memory
    alpha: 32                # LoRA alpha (typically 2*r)
    dropout: 0.05            # LoRA dropout
    target_modules:          # Modules to apply LoRA to
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
  
  # Training hyperparameters
  num_epochs: 5              # 5 epochs for 7B model
  micro_batch_size: 1        # Minimum batch for memory
  gradient_accumulation_steps: 4   # Further reduced (effective batch = 1*4*8 = 32)
  eval_batch_size: 1         # Small eval batch
  learning_rate: 0.0001      # 1e-4 for 7B
  lr_scheduler: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.1
  max_grad_norm: 0.5
  
  # Precision
  bf16: true
  fp16: false
  tf32: true
  
  # Logging & Checkpointing
  logging_steps: 10
  save_steps: 50             # Save checkpoint every 50 steps (keep last 3)
  eval_steps: 50             # Evaluate every 50 steps
  
  # Memory optimization
  gradient_checkpointing: true
  sample_packing: false
  
# Dataset Configuration
dataset:
  output_dir: "/home/aditya.narayan/TPU_FineTuning/dataset"
  train_split: 0.90
  val_split: 0.10
  random_seed: 42
  
  # Context window settings
  max_tokens: 1024          # Reduced to 1024 for maximum memory safety
  overlap_tokens: 64        # Smaller overlap
  
  # Dataset modes - for Q&A and code understanding
  modes:
    - "full_file"           # Complete files with context
    - "function_focus"      # Individual functions with context
    - "dependency_map"      # File dependencies and imports
    - "api_catalog"         # Function signatures and docs
  
  # Filtering rules
  include:
    - "*.rs"                # Include all Rust files
    - "!**/target/**"       # Exclude build artifacts
    - "!**/*.gen.rs"        # Exclude generated files
    - "!**/generated/**"    # Exclude generated directory
  
  # Test files strategy
  include_tests: true       # Tests show usage patterns - include them
  
  # Data augmentation for Q&A tasks
  augmentation:
    add_file_path: true         # Prefix with file path
    add_module_context: true    # Add import context
    extract_functions: true     # Extract individual functions
    extract_structs: true       # Extract struct definitions
    extract_traits: true        # Extract trait definitions
    add_dependencies: true      # Add import/use statements
    add_cross_references: true  # Add related file references
    
# Preprocessing Configuration
preprocessing:
  # What to keep
  keep_comments: true       # Keep documentation and important comments
  keep_docstrings: true     # Keep /// and //! comments
  keep_attributes: true     # Keep #[derive(...)] etc.
  
  # What to remove/clean
  remove_debug_prints: false  # Keep for now, they show debugging patterns
  normalize_whitespace: true  # Normalize excessive blank lines
  max_blank_lines: 2         # Max consecutive blank lines
  
  # Token counting
  tokenizer: "Qwen/Qwen2.5-Coder-7B-Instruct"

# Output format
output:
  format: "jsonl"           # JSONL format for training
  fields:
    - "text"                # Main text field
    - "file_path"           # Source file path
    - "module"              # Crate/module name
    - "tokens"              # Token count
    - "type"                # Sample type (full_file, function, etc.)
    - "metadata"            # Additional metadata (functions, structs, deps)
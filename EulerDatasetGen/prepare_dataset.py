#!/usr/bin/env python3
"""
Optimized dataset preparation - writes incrementally to avoid memory issues
"""

import json
import yaml
import re
from pathlib import Path
from typing import List, Dict
import random
from tqdm import tqdm
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class StreamingHaskellDatasetPreparator:
    """
    Streaming dataset preparator - processes and writes incrementally
    Much faster and more memory efficient
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """Initialize with configuration"""
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.repo_paths = [Path(p) for p in self.config['repository']['paths']]
        self.output_dir = Path(self.config['dataset']['output_dir'])
        self.max_tokens = self.config['dataset']['max_tokens']
        self.overlap_tokens = self.config['dataset']['overlap_tokens']
        
        # Create output directories
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Will be set when tokenizer is loaded
        self.tokenizer = None
        
        # Statistics
        self.stats = {
            'file_count': 0,
            'file_samples': 0,
            'granular_samples': 0,
            'total_tokens': 0,
            'sample_types': {},
            'modules': set()
        }
        
    def load_tokenizer(self):
        """Load tokenizer for token counting"""
        try:
            from transformers import AutoTokenizer
            tokenizer_name = self.config['model']['name']
            logger.info(f"Loading tokenizer: {tokenizer_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_name,
                trust_remote_code=True
            )
            logger.info("Tokenizer loaded successfully")
        except Exception as e:
            logger.warning(f"Could not load tokenizer: {e}")
            logger.warning("Using character-based estimation (1 token â‰ˆ 4 chars)")
            self.tokenizer = None
    
    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        if self.tokenizer:
            return len(self.tokenizer.encode(text, add_special_tokens=False))
        else:
            return len(text) // 4
    
    def is_excluded_path(self, path: Path) -> bool:
        """Check if path should be excluded"""
        path_str = str(path)
        exclude_patterns = ['/dist-newstyle/', '/.stack-work/', '/.cabal/', '/generated/', '.gen.hs']
        return any(pattern in path_str for pattern in exclude_patterns)
    
    def should_skip_file(self, path: Path, content: str) -> bool:
        """Check if file should be skipped"""
        # Skip tests if configured
        if not self.config['dataset'].get('include_tests', False):
            path_str = str(path)
            if '/test/' in path_str or path_str.endswith('_test.hs'):
                return True
            test_markers = ['Test.Framework', 'Test.Hspec', 'Test.QuickCheck', 'Spec']
            if any(marker in content for marker in test_markers):
                return True
        
        # Skip generated files
        first_lines = '\n'.join(content.split('\n')[:10])
        generation_markers = ['generated by', 'auto-generated', 'this file is generated', 'do not edit']
        return any(marker in first_lines.lower() for marker in generation_markers)
    
    def clean_content(self, content: str, is_literate: bool = False) -> str:
        """Clean and normalize content"""
        content = content.replace('\r\n', '\n')
        content = re.sub(r'\n{3,}', '\n\n', content)
        if is_literate:
            lines = []
            for line in content.split('\n'):
                if line.startswith('> '):
                    lines.append(line[2:])
                elif line.startswith('>'):
                    lines.append(line[1:])
            content = '\n'.join(lines)
        lines = [line.rstrip() for line in content.split('\n')]
        return '\n'.join(lines)
    
    def extract_module_name(self, rel_path: Path) -> str:
        """Extract module name from path"""
        parts = rel_path.parts
        if len(parts) > 1:
            return '.'.join(parts[:-1]).replace('/', '.')
        return "Main"
    
    def process_file(self, file_path: Path, repo_path: Path, output_file) -> int:
        """Process a single file and write samples immediately"""
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            rel_path = file_path.relative_to(repo_path)
            
            if self.should_skip_file(file_path, content):
                return 0
            
            is_literate = file_path.suffix == '.lhs'
            content = self.clean_content(content, is_literate)
            module = self.extract_module_name(rel_path)
            
            self.stats['modules'].add(module)
            samples_written = 0
            
            # Write file-level samples
            tokens = self.count_tokens(content)
            
            if tokens <= self.max_tokens:
                sample = {
                    'text': content,
                    'file_path': str(rel_path),
                    'module': module,
                    'tokens': tokens,
                    'type': 'full_file'
                }
                output_file.write(json.dumps(sample, ensure_ascii=False) + '\n')
                self.stats['file_samples'] += 1
                self.stats['total_tokens'] += tokens
                self.stats['sample_types']['full_file'] = self.stats['sample_types'].get('full_file', 0) + 1
                samples_written += 1
            else:
                # Chunk large files
                lines = content.split('\n')
                current_chunk = []
                current_tokens = 0
                chunk_idx = 0
                
                for line in lines:
                    line_tokens = self.count_tokens(line + '\n')
                    
                    if current_tokens + line_tokens > self.max_tokens and current_chunk:
                        chunk_text = '\n'.join(current_chunk)
                        sample = {
                            'text': chunk_text,
                            'file_path': f"{rel_path}#chunk{chunk_idx}",
                            'module': module,
                            'tokens': current_tokens,
                            'type': 'chunk'
                        }
                        output_file.write(json.dumps(sample, ensure_ascii=False) + '\n')
                        self.stats['file_samples'] += 1
                        self.stats['total_tokens'] += current_tokens
                        self.stats['sample_types']['chunk'] = self.stats['sample_types'].get('chunk', 0) + 1
                        samples_written += 1
                        
                        overlap_lines = current_chunk[-20:] if len(current_chunk) > 20 else current_chunk
                        current_chunk = overlap_lines
                        current_tokens = self.count_tokens('\n'.join(overlap_lines))
                        chunk_idx += 1
                    
                    current_chunk.append(line)
                    current_tokens += line_tokens
                
                if current_chunk:
                    chunk_text = '\n'.join(current_chunk)
                    sample = {
                        'text': chunk_text,
                        'file_path': f"{rel_path}#chunk{chunk_idx}",
                        'module': module,
                        'tokens': current_tokens,
                        'type': 'chunk'
                    }
                    output_file.write(json.dumps(sample, ensure_ascii=False) + '\n')
                    self.stats['file_samples'] += 1
                    self.stats['total_tokens'] += current_tokens
                    self.stats['sample_types']['chunk'] = self.stats['sample_types'].get('chunk', 0) + 1
                    samples_written += 1
            
            # Extract granular samples (functions and data types only - faster)
            # Functions
            pattern = r'^\s*(\w+)\s*::\s*([^\n]+)\n'
            for match in re.finditer(pattern, content, re.MULTILINE):
                fn_name = match.group(1)
                type_sig = match.group(2).strip()
                
                text = f"{fn_name} :: {type_sig}"
                tokens = self.count_tokens(text)
                
                if tokens < self.max_tokens:
                    sample = {
                        "text": text,
                        "file_path": str(rel_path),
                        "module": module,
                        "type": "function_sig",
                        "function_name": fn_name,
                        "tokens": tokens
                    }
                    output_file.write(json.dumps(sample, ensure_ascii=False) + '\n')
                    self.stats['granular_samples'] += 1
                    self.stats['total_tokens'] += tokens
                    self.stats['sample_types']['function_sig'] = self.stats['sample_types'].get('function_sig', 0) + 1
                    samples_written += 1
            
            self.stats['file_count'] += 1
            return samples_written
            
        except Exception as e:
            logger.debug(f"Error processing {file_path}: {e}")
            return 0
    
    def collect_haskell_files(self) -> List[tuple]:
        """Collect all Haskell files"""
        logger.info(f"Collecting Haskell files from {len(self.repo_paths)} repositories")
        haskell_files = []
        for repo_path in self.repo_paths:
            hs_files = list(repo_path.rglob("*.hs")) + list(repo_path.rglob("*.lhs"))
            for f in hs_files:
                if not self.is_excluded_path(f):
                    haskell_files.append((f, repo_path))
        logger.info(f"Found {len(haskell_files)} Haskell files")
        return haskell_files
    
    def prepare(self):
        """Main preparation pipeline"""
        logger.info("=" * 70)
        logger.info("Streaming Dataset Preparation for Haskell CPT")
        logger.info("=" * 70)
        
        self.load_tokenizer()
        
        haskell_files = self.collect_haskell_files()
        
        if not haskell_files:
            logger.error("No Haskell files found!")
            return
        
        # Open output file and process streaming
        output_path = self.output_dir / 'temp_data.jsonl'
        logger.info(f"Processing files and writing to {output_path}...")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for file_path, repo_path in tqdm(haskell_files, desc="Processing", unit="files"):
                self.process_file(file_path, repo_path, f)
        
        logger.info(f"Processed {self.stats['file_count']} files")
        logger.info(f"Created {self.stats['file_samples']} file-level samples")
        logger.info(f"Created {self.stats['granular_samples']} granular samples")
        
        # Shuffle the dataset
        logger.info("Shuffling dataset...")
        final_output = self.output_dir / self.config['dataset']['data_file']
        
        with open(output_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        random.seed(self.config['dataset']['random_seed'])
        random.shuffle(lines)
        
        with open(final_output, 'w', encoding='utf-8') as f:
            f.writelines(lines)
        
        # Remove temp file
        output_path.unlink()
        
        # Save stats
        stats_dict = {
            'total_samples': self.stats['file_samples'] + self.stats['granular_samples'],
            'file_level_samples': self.stats['file_samples'],
            'granular_samples': self.stats['granular_samples'],
            'total_tokens': self.stats['total_tokens'],
            'sample_types': self.stats['sample_types'],
            'modules': list(self.stats['modules']),
            'files_processed': self.stats['file_count']
        }
        
        stats_path = self.output_dir / "dataset_stats.json"
        with open(stats_path, 'w') as f:
            json.dump(stats_dict, f, indent=2)
        
        logger.info(f"\n{'='*70}")
        logger.info("Dataset Summary:")
        logger.info(f"{'='*70}")
        logger.info(f"  Total samples: {stats_dict['total_samples']:,}")
        logger.info(f"    - File-level: {stats_dict['file_level_samples']:,}")
        logger.info(f"    - Granular: {stats_dict['granular_samples']:,}")
        logger.info(f"  Total tokens: {stats_dict['total_tokens']:,}")
        logger.info(f"  Modules: {len(stats_dict['modules'])}")
        logger.info(f"  Files processed: {stats_dict['files_processed']}")
        logger.info(f"{'='*70}")
        logger.info(f"\nDataset saved to: {final_output}")
        logger.info(f"Stats saved to: {stats_path}")
        logger.info(f"{'='*70}")


def main():
    preparator = StreamingHaskellDatasetPreparator("config.yaml")
    preparator.prepare()


if __name__ == "__main__":
    main()

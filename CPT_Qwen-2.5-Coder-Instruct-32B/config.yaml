# CPT Configuration for Hyperswitch Rust Repository
# =================================================

# Repository Configuration
repository:
  name: "hyperswitch"
  path: "/workspace/RL_Code/CPT/hyperswitch"
  language: "rust"

# Model Configuration
model:
  name: "Qwen/Qwen2.5-Coder-32B-Instruct"
  size: "32B"
  type: "instruct"  # instruct model for better prompting

# Training Configuration
training:
  # LoRA settings
  lora:
    r: 64 #64                    # LoRA rank (64 or 128)
    alpha: 128 #128              # LoRA alpha (typically 2*r)
    dropout: 0.05            # LoRA dropout
    target_modules:          # Modules to apply LoRA to
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
  
  # Training hyperparameters
  num_epochs: 5 #5
  micro_batch_size: 2 #2
  gradient_accumulation_steps: 8 #8
  eval_batch_size: 1 #2
  learning_rate: 0.00005     # 2e-5 , 5e-5
  lr_scheduler: "cosine"
  warmup_ratio: 0.02 #0.05
  weight_decay: 0.1 #0.0
  max_grad_norm: 0.5
  
  # Precision
  bf16: true
  fp16: false
  tf32: true
  
  # Logging & Checkpointing
  logging_steps: 10
  save_steps: 50
  eval_steps: 25
  
  # Memory optimization
  gradient_checkpointing: true
  sample_packing: false
  
# Dataset Configuration
dataset:
  output_dir: "/workspace/RL_Code/CPT/dataset"
  train_split: 0.90
  val_split: 0.10
  random_seed: 42
  
  # Context window settings
  max_tokens: 8192          # Max context length for model
  overlap_tokens: 512       # Overlap for sliding window on long files
  
  # Dataset modes - for Q&A and code understanding
  modes:
    - "full_file"           # Complete files with context
    - "function_focus"      # Individual functions with context
    - "dependency_map"      # File dependencies and imports
    - "api_catalog"         # Function signatures and docs
  
  # Filtering rules
  include:
    - "*.rs"                # Include all Rust files
    - "!**/target/**"       # Exclude build artifacts
    - "!**/*.gen.rs"        # Exclude generated files
    - "!**/generated/**"    # Exclude generated directory
  
  # Test files strategy
  include_tests: true       # Tests show usage patterns - include them
  
  # Data augmentation for Q&A tasks
  augmentation:
    add_file_path: true         # Prefix with file path
    add_module_context: true    # Add import context
    extract_functions: true     # Extract individual functions
    extract_structs: true       # Extract struct definitions
    extract_traits: true        # Extract trait definitions
    add_dependencies: true      # Add import/use statements
    add_cross_references: true  # Add related file references
    
# Preprocessing Configuration
preprocessing:
  # What to keep
  keep_comments: true       # Keep documentation and important comments
  keep_docstrings: true     # Keep /// and //! comments
  keep_attributes: true     # Keep #[derive(...)] etc.
  
  # What to remove/clean
  remove_debug_prints: false  # Keep for now, they show debugging patterns
  normalize_whitespace: true  # Normalize excessive blank lines
  max_blank_lines: 2         # Max consecutive blank lines
  
  # Token counting
  tokenizer: "Qwen/Qwen2.5-Coder-32B-Instruct"

# Output format
output:
  format: "jsonl"           # JSONL format for training
  fields:
    - "text"                # Main text field
    - "file_path"           # Source file path
    - "module"              # Crate/module name
    - "tokens"              # Token count
    - "type"                # Sample type (full_file, function, etc.)
    - "metadata"            # Additional metadata (functions, structs, deps)

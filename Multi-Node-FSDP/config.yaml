# FSDP Training Configuration

# Model Configuration
model:
  # Can be HuggingFace repo name (e.g., "Qwen/Qwen2.5-7B") or local path (e.g., "/workspace/models/my-model")
  name_or_path: "/workspace/Avinash/models/GLM-4.5-Air"
  
  # Trust remote code for models that require it (e.g., GLM, Qwen)
  trust_remote_code: true
  
  # Optional: specify custom HuggingFace cache directory
  # If not set, uses default ~/.cache/huggingface
  cache_dir: null

# Data Configuration
data:
  # Path to JSONL dataset file
  dataset_path: "/workspace/Avinash/dataset/all_data.jsonl"
  
  # Name of text column in JSONL
  text_column: "text"
  
  # Maximum sequence length (tokens will be truncated/padded to this length)
  max_length: 16384  # ðŸ”¥ Testing 16k!
  
  # Validation split ratio (0.0 to 1.0)
  val_split: 0.1

# Training Configuration
training:
  # Output directory for checkpoints and logs
  output_dir: "./glm_fsdp_output"
  
  # Random seed for reproducibility
  seed: 42
  
  # Number of training epochs
  num_train_epochs: 2                    # Full training run
  
  # Batch size per GPU
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  
  # Gradient accumulation steps (effective batch = per_device_batch * accumulation * num_gpus)
  gradient_accumulation_steps: 2
  
  # Learning rate
  learning_rate: 5.0e-5
  
  # Weight decay for regularization
  weight_decay: 0.01
  
  # Warmup ratio (fraction of total steps for warmup)
  warmup_ratio: 0.1
  
  # Logging frequency (steps)
  logging_steps: 1
  
  # Evaluation strategy: "steps" or "epoch"
  eval_strategy: "steps"
  
  # Evaluation configuration
  eval_dataset_type: "subset"  # "subset" or "full"
  eval_subset_size: 200  # Number of samples for subset evaluation (ignored if eval_dataset_type="full")
  
  eval_steps: 50  # PRODUCTION: Evaluate every 50 steps
  
  # Checkpointing frequency (steps)
  save_steps: 100  # PRODUCTION: Save every 100 steps
  
  # Checkpoint mode: "lite" (adapters only, fast) or "full" (optimizer+adapters, slow)
  checkpoint_mode: "lite"  # Sharded LoRA adapters (~256MB total)
  
  # Enable baseline evaluation before training starts
  run_baseline_eval: true  # Skip baseline for faster start
  
  # Checkpoint management
  save_total_limit: 10  # PRODUCTION: Keep last 10 checkpoints
  
  # Use bfloat16 mixed precision (recommended for H100/A100)
  bf16: true
  
  # Enable gradient checkpointing to reduce memory usage
  gradient_checkpointing: true

# LoRA Configuration
lora:
  # LoRA rank (higher = more parameters)
  r: 64
  
  # LoRA alpha (scaling factor, typically 2*r)
  lora_alpha: 128
  
  # LoRA dropout
  lora_dropout: 0.1
  
  # Bias training: "none", "all", or "lora_only"
  bias: "none"
  
  # Target modules for LoRA (attention projections are standard)
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    # For MLP as well (more parameters but better performance):
    # - "gate_proj"
    # - "up_proj"
    # - "down_proj"

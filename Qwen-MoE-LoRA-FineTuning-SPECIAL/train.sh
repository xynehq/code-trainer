#!/bin/bash
# ============================================================================
# MoE Fine-tuning Training Launcher
# Distributed training across 4x H200 GPUs
# ============================================================================

set -e  # Exit on error

echo "============================================================================"
echo "üöÄ MoE Fine-tuning Training Launcher"
echo "============================================================================"

# Configuration
export CUDA_VISIBLE_DEVICES=0,1,2,3
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=0
export NCCL_NET_GDR_LEVEL=5
export TOKENIZERS_PARALLELISM=false

# Paths
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_PATH="${SCRIPT_DIR}/config.yaml"
TRAIN_SCRIPT="${SCRIPT_DIR}/train_moe.py"

# Check if virtual environment is activated
if [[ -z "${VIRTUAL_ENV}" ]]; then
    echo "‚ö†Ô∏è  Virtual environment not activated. Activating..."
    source "${SCRIPT_DIR}/.venv/bin/activate"
fi

# Check if config exists
if [[ ! -f "$CONFIG_PATH" ]]; then
    echo "‚ùå Config file not found: $CONFIG_PATH"
    exit 1
fi

# Check if training script exists
if [[ ! -f "$TRAIN_SCRIPT" ]]; then
    echo "‚ùå Training script not found: $TRAIN_SCRIPT"
    exit 1
fi

# Check GPU availability
echo ""
echo "üìä GPU Information:"
nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv,noheader
echo ""

# Number of GPUs
NUM_GPUS=$(nvidia-smi --list-gpus | wc -l)
echo "üéÆ Detected $NUM_GPUS GPUs"

# Check if dataset exists
DATA_FILE="${SCRIPT_DIR}/data/all_data.jsonl"
if [[ ! -f "$DATA_FILE" ]]; then
    echo "‚ùå Dataset not found: $DATA_FILE"
    echo "üìù Please run prepare_dataset.py first:"
    echo "   python prepare_dataset.py"
    exit 1
fi

echo ""
echo "‚úì Configuration: $CONFIG_PATH"
echo "‚úì Training script: $TRAIN_SCRIPT"
echo "‚úì Dataset: $DATA_FILE"
echo ""

# Launch training with accelerate or torchrun
echo "============================================================================"
echo "üöÇ Launching distributed training on $NUM_GPUS GPUs..."
echo "============================================================================"
echo ""

# Option 1: Using accelerate (recommended)
if command -v accelerate &> /dev/null; then
    echo "Using Accelerate for distributed training..."
    accelerate launch \
        --num_processes=$NUM_GPUS \
        --multi_gpu \
        --mixed_precision=bf16 \
        "$TRAIN_SCRIPT" \
        --config "$CONFIG_PATH"

# Option 2: Using torchrun (fallback)
elif command -v torchrun &> /dev/null; then
    echo "Using torchrun for distributed training..."
    torchrun \
        --nproc_per_node=$NUM_GPUS \
        --nnodes=1 \
        --node_rank=0 \
        --master_addr=localhost \
        --master_port=29500 \
        "$TRAIN_SCRIPT" \
        --config "$CONFIG_PATH"

# Option 3: Single GPU fallback
else
    echo "‚ö†Ô∏è  No distributed launcher found. Running on single GPU..."
    python "$TRAIN_SCRIPT" --config "$CONFIG_PATH"
fi

# Check exit status
if [[ $? -eq 0 ]]; then
    echo ""
    echo "============================================================================"
    echo "‚úÖ Training completed successfully!"
    echo "============================================================================"
    echo ""
    echo "üìÅ Output directory: outputs/moe-hyperswitch-attn-lora"
    echo "üìä Tensorboard logs: outputs/moe-hyperswitch-attn-lora/runs"
    echo ""
    echo "To view training logs:"
    echo "  tensorboard --logdir outputs/moe-hyperswitch-attn-lora"
    echo ""
else
    echo ""
    echo "============================================================================"
    echo "‚ùå Training failed!"
    echo "============================================================================"
    exit 1
fi
